{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe810e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Polina\\Desktop\\envs\\project_2048\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_2048 \n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from simulations.base_agents import DQNAgentWrapper\n",
    "from simulations.interpretable_agents import ViperAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0dd26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"gymnasium_2048/TwentyFortyEight-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04facbe",
   "metadata": {},
   "source": [
    "## VIPER agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d4970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = DQNAgentWrapper(\"env_2048/save/best/dqn_7710.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76cc0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VIPER it:   0%|          | 0/15 [00:00<?, ?it/s]c:\\Users\\Polina\\Desktop\\envs\\project_2048\\lib\\site-packages\\gymnasium_2048\\envs\\twenty_forty_eight.py:250: RuntimeWarning: overflow encountered in scalar add\n",
      "  self.total_score += self.step_score\n",
      "VIPER it:   0%|          | 0/15 [05:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(ENV_ID) \n\u001b[1;32m----> 3\u001b[0m viper \u001b[38;5;241m=\u001b[39m \u001b[43mViperAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_trajs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Polina\\Desktop\\code\\project_2048\\simulations\\interpretable_agents.py:102\u001b[0m, in \u001b[0;36mViperAgent.__init__\u001b[1;34m(self, env, oracle, num_iters, num_trajs, max_depth, min_samples_leaf, random_state, auto_train)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree: Optional[DecisionTreeClassifier] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_train:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Polina\\Desktop\\code\\project_2048\\simulations\\interpretable_agents.py:112\u001b[0m, in \u001b[0;36mViperAgent.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m it_iter \u001b[38;5;241m=\u001b[39m trange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIPER it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _it \u001b[38;5;129;01min\u001b[39;00m it_iter:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_trajs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_tree()\n\u001b[0;32m    114\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(tree, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Polina\\Desktop\\code\\project_2048\\simulations\\interpretable_agents.py:162\u001b[0m, in \u001b[0;36mViperAgent._collect_dataset\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m    159\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Oracle guidance ------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oracle_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     oracle_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(q_values))\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Importance weight \\tilde ℓ(s) = max_a Q – min_a Q\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Polina\\Desktop\\code\\project_2048\\simulations\\interpretable_agents.py:242\u001b[0m, in \u001b[0;36mViperAgent._oracle_q_values\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the oracle's Q‑values for the given observation (shape = 4).\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m board \u001b[38;5;241m=\u001b[39m get_tile_value(obs)\n\u001b[1;32m--> 242\u001b[0m state_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_board\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    244\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mnet(state_t)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\Polina\\Desktop\\code\\project_2048\\simulations\\base_agents.py:69\u001b[0m, in \u001b[0;36mDQNAgentWrapper._preprocess_board\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mboard (4×4, реальные числа) → 1×1×4×4 tensor; масштаб как в оригинале:\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mlog₂(value+1) / 16  (см. README и utils.py репозитория).”\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(board \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, where\u001b[38;5;241m=\u001b[39mboard \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16.0\u001b[39m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_ID) \n",
    "\n",
    "viper = ViperAgent(\n",
    "    env,\n",
    "    oracle,\n",
    "    num_iters=15,        \n",
    "    num_trajs=80,        \n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f03b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Инициализация VIPER-агента\n",
    "# ORACLE_PATH = 'drl2048/trained_architectures/convdqn_torch_300_net.pt'\n",
    "# env = gym.make(ENV_ID)\n",
    "# viper = ViperAgent(\n",
    "#     env=env,\n",
    "#     oracle_path=ORACLE_PATH,\n",
    "#     num_rollouts=500,      # Количество эпизодов для сбора данных\n",
    "#     max_depth=12,           # Максимальная глубина дерева\n",
    "#     batch_size=128,         # Размер батча для обучения\n",
    "#     gamma=0.95              # Коэффициент дисконтирования\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viper.collect_data()   # tqdm-полоса «VIPER Rollouts»\n",
    "# viper.train()          # ещё один tqdm «Preparing training data»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # создаём псевдонимы для старых импорт-путей внутри pickle\n",
    "# sys.modules['modules'] = real_architectures.__package__ and __import__('drl2048.modules', fromlist=['']) or real_architectures\n",
    "# sys.modules['modules.architectures'] = real_architectures\n",
    "\n",
    "# class ViperAgent(BaseAgent):\n",
    "#     \"\"\"VIPER agent с извлечением стратегии через имитационное обучение\"\"\"\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         env,\n",
    "#         oracle_path,\n",
    "#         num_rollouts=500,\n",
    "#         max_depth=10,\n",
    "#         batch_size=64,\n",
    "#         gamma=0.99\n",
    "#     ):\n",
    "#         super().__init__(env)\n",
    "        \n",
    "#         # Параметры обучения - define these FIRST\n",
    "#         self.num_rollouts = num_rollouts\n",
    "#         self.max_depth = max_depth\n",
    "#         self.batch_size = batch_size\n",
    "#         self.gamma = gamma\n",
    "        \n",
    "#         # Then load the oracle model\n",
    "#         self.oracle = self._load_oracle(oracle_path)\n",
    "#         self.oracle.eval()\n",
    "        \n",
    "#         # Буферы данных\n",
    "#         self.dataset = deque(maxlen=100000)\n",
    "#         self.tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "        \n",
    "#         # Конвертер состояний\n",
    "#         self.tile_values = [2**i for i in range(1, 16)]\n",
    "        \n",
    "#     def _load_oracle(\n",
    "#         self, \n",
    "#         path: str, \n",
    "#         agent_params={\n",
    "#             'gamma': 0.995,\n",
    "#             'replay_memory_size': 10000,\n",
    "#             'batch_size': 384,\n",
    "#             'eps_start': 0.8,\n",
    "#             'eps_end': 0.02, \n",
    "#             'eps_decay': 1000,             \n",
    "#             'tau': 5e-3,\n",
    "#             'kind_action': \"entropy\",\n",
    "#             'lr': 1e-3\n",
    "#         }, \n",
    "#         model_params={'l2_regularization': 3e-4}\n",
    "#     ) -> nn.Module:\n",
    "#         return ConvDQNAgentWrapper(model_path=path, agent_params=agent_params, model_params=model_params).model\n",
    "    \n",
    "#     def _state_to_tensor(self, board: np.ndarray):\n",
    "#         \"\"\"board 4×4 -> tensor 1×12×4×4, без Python-циклов\"\"\"\n",
    "#         # board → экспоненты (-1, 0 … 11)  | -1 = пустая клетка\n",
    "#         exps = np.where(board > 0, np.clip(np.log2(board).astype(int) - 1, 0, 11), -1)\n",
    "#         # one-hot: (4,4,12)\n",
    "#         onehot = np.eye(12, dtype=np.float32)[exps.clip(0)]       # пустые ячейки = канал 0\n",
    "#         tensor = torch.from_numpy(onehot.transpose(2,0,1))        # → (12,4,4)\n",
    "#         return tensor.unsqueeze(0).to(self.device)\n",
    "    \n",
    "#     def collect_data(self):\n",
    "#         \"\"\"Сбор данных траекторий oracle-сети для VIPER.\"\"\"\n",
    "#         for _ in tqdm(range(self.num_rollouts), desc=\"VIPER Rollouts\"):\n",
    "#             obs, _ = self.env.reset()\n",
    "#             done = False\n",
    "#             while not done:\n",
    "#                 board = get_tile_value(obs)                 # [НОВОЕ]\n",
    "#                 state_tensor = self._state_to_tensor(board) # (а не raw obs)\n",
    "#                 with torch.no_grad():\n",
    "#                     q_values = self.oracle(state_tensor).cpu().numpy()[0]\n",
    "\n",
    "#                 # advantage gap\n",
    "#                 advantage = np.partition(q_values, -2)[-1] - np.partition(q_values, -2)[-2]\n",
    "\n",
    "#                 # interpretable features от **numeric** board\n",
    "#                 features = extract_features(board)\n",
    "\n",
    "#                 self.dataset.append({\n",
    "#                     \"features\": list(features.values()),\n",
    "#                     \"action\": int(q_values.argmax()),\n",
    "#                     \"weight\": advantage ** 2,\n",
    "#                 })\n",
    "\n",
    "#                 obs, _, done, _, _ = self.env.step(int(q_values.argmax()))\n",
    "\n",
    "#     def _state_to_tensor(self, state: np.ndarray):\n",
    "#         \"\"\"Convert raw env obs (4×4×17) **или** numeric board (4×4) → 1×12×4×4 binary tensor.\"\"\"\n",
    "#         if state.ndim == 3:                       # [НОВОЕ] пришёл raw-тензор из среды\n",
    "#             state = get_tile_value(state)         # -> (4, 4) с числами 0/2/4/…\n",
    "\n",
    "#         tensor = torch.zeros(12, 4, 4, device=device)\n",
    "#         for i in range(4):\n",
    "#             for j in range(4):\n",
    "#                 val = state[i, j]\n",
    "#                 if val == 0:\n",
    "#                     continue\n",
    "#                 exp = min(int(np.log2(val)) - 1, 11)   # всё, что ≥4096, уезжает в последний канал\n",
    "#                 tensor[exp, i, j] = 1\n",
    "#         return tensor.unsqueeze(0)\n",
    "    \n",
    "#     def train(self):\n",
    "#         \"\"\"Обучение дерева решений с учётом весов + индикатор прогресса\"\"\"\n",
    "#         if not self.dataset:\n",
    "#             raise ValueError(\"Сначала выполните сбор данных через collect_data()\")\n",
    "\n",
    "#         # —–– Подготовка массивов с progress-bar –––\n",
    "#         X, y, weights = [], [], []\n",
    "#         for sample in tqdm(self.dataset,\n",
    "#                            desc=\"Preparing training data\",\n",
    "#                            unit=\"samples\"):\n",
    "#             X.append(sample['features'])\n",
    "#             y.append(sample['action'])\n",
    "#             weights.append(sample['weight'])\n",
    "\n",
    "#         X = np.asarray(X, dtype=np.float32)\n",
    "#         y = np.asarray(y, dtype=np.int64)\n",
    "#         weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "#         # —–– Обучение дерева (одиночным шагом) –––\n",
    "#         # tqdm.write выводит сообщение, не «ломая» полосу прогресса\n",
    "#         tqdm.write(\"Fitting decision tree…\")\n",
    "#         self.tree.fit(X, y, sample_weight=weights)\n",
    "#         tqdm.write(\"Decision tree training complete ✅\")\n",
    "    \n",
    "#     def act(self, observation):\n",
    "#         \"\"\"Действие по обученному дереву решений.\"\"\"\n",
    "#         board = get_tile_value(observation) if observation.ndim == 3 else observation\n",
    "#         feats = extract_features(board)\n",
    "#         return int(self.tree.predict([list(feats.values())])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SindyAgent(BaseAgent):\n",
    "#     \"\"\"\n",
    "#     Интерпретируемая SINDy-политика:\n",
    "#     1) собирает rollout’ы у обученного DQN-оракула,\n",
    "#     2) извлекает фичи φ(s) = extract_features(board),\n",
    "#     3) строит полиномиальную библиотеку признаков Poly(φ),\n",
    "#     4) обучает 4 LASSO-регрессора Q̂_a(φ) для каждого действия a.\n",
    "#     При act выбирает a = argmax_a Q̂_a(φ(s)), игнорируя нелегальные ходы.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         env,\n",
    "#         oracle_path: str,\n",
    "#         num_rollouts: int = 200,\n",
    "#         poly_order: int = 2,\n",
    "#         alpha: float = 0.01\n",
    "#     ):\n",
    "#         super().__init__(env)\n",
    "#         # 1) оракул\n",
    "#         self.oracle = ConvDQNAgentWrapper(oracle_path)\n",
    "#         self.num_rollouts = num_rollouts\n",
    "#         self.poly_order = poly_order\n",
    "#         self.alpha = alpha\n",
    "#         # 2) сбор данных + обучение\n",
    "#         X, Y = self._collect_data()\n",
    "#         self._fit_sindy_policy(X, Y)\n",
    "\n",
    "#     def _collect_data(self):\n",
    "#         \"\"\"Запускает несколько игр у oracle, собирает φ(s) и истинные Q(s,a).\"\"\"\n",
    "#         X_list, Y_list = [], []\n",
    "#         for _ in tqdm(range(self.num_rollouts)):\n",
    "#             obs, _ = self.env.reset()\n",
    "#             done = False\n",
    "#             while not done:\n",
    "#                 board = get_tile_value(obs)\n",
    "#                 φ = extract_features(board)\n",
    "#                 # Q-ценности из оракула\n",
    "#                 state_t = ConvDQNAgentWrapper._encode_board(board).unsqueeze(0).to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     q_vals = self.oracle.agent.model(state_t).cpu().numpy().ravel()\n",
    "#                 X_list.append(φ)\n",
    "#                 Y_list.append(q_vals)\n",
    "#                 # шаг по оракулу\n",
    "#                 a = int(np.argmax(q_vals))\n",
    "#                 obs, _, terminated, truncated, _ = self.env.step(a)\n",
    "#                 done = terminated or truncated\n",
    "#         X = np.vstack(X_list)               # shape=(N, n_features)\n",
    "#         Y = np.vstack(Y_list)               # shape=(N, 4)\n",
    "#         return X, Y\n",
    "\n",
    "#     def _fit_sindy_policy(self, X: np.ndarray, Y: np.ndarray):\n",
    "#         \"\"\"Строим полиномиальную библиотеку и обучаем 4 LASSO-регрессора.\"\"\"\n",
    "#         poly = PolynomialFeatures(degree=self.poly_order, include_bias=False)\n",
    "#         Xp = poly.fit_transform(X)\n",
    "#         models = []\n",
    "#         for a in range(Y.shape[1]):\n",
    "#             lasso = Lasso(alpha=self.alpha, max_iter=10000)\n",
    "#             lasso.fit(Xp, Y[:, a])\n",
    "#             models.append(lasso)\n",
    "#         self.poly = poly\n",
    "#         self.models = models\n",
    "#         # для интерпретации можно сохранить имена признаков:\n",
    "#         self.term_names = poly.get_feature_names_out(feature_names)\n",
    "\n",
    "#     def act(self, obs: np.ndarray) -> int:\n",
    "#         board = get_tile_value(obs)\n",
    "#         φ = extract_features(board).reshape(1, -1)\n",
    "#         Xp = self.poly.transform(φ)\n",
    "#         q_pred = np.array([m.predict(Xp)[0] for m in self.models])\n",
    "#         # запрещаем нелегальные ходы\n",
    "#         legal = ConvDQNAgentWrapper._legal_moves(board)\n",
    "#         q_pred[~legal] = -np.inf\n",
    "#         return int(np.argmax(q_pred))\n",
    "    \n",
    "#     def _action_name(self, idx: int) -> str:\n",
    "#         \"\"\"Удобное текстовое имя действия по индексу 0…3.\"\"\"\n",
    "#         return [\"up\", \"down\", \"left\", \"right\"][idx]\n",
    "    \n",
    "#     def get_symbolic_policy(self, threshold: float = 1e-5) -> dict:\n",
    "#         \"\"\"\n",
    "#         Возвращает разреженное представление политики в виде:\n",
    "#         {\n",
    "#             'up':   { 'empty_ratio':  +0.42, 'max_tile_ratio': -0.37, ... },\n",
    "#             'down': { ... },\n",
    "#             ...\n",
    "#         }\n",
    "#         Включаются только коэффициенты |coef| >= threshold.\n",
    "#         \"\"\"\n",
    "#         if not hasattr(self, \"models\"):\n",
    "#             raise RuntimeError(\"SindyAgent ещё не обучен (models отсутствуют).\")\n",
    "\n",
    "#         policy = {}\n",
    "#         for a, model in enumerate(self.models):\n",
    "#             nonzero = np.abs(model.coef_) >= threshold\n",
    "#             terms = {\n",
    "#                 term: float(coef)\n",
    "#                 for coef, term in zip(model.coef_[nonzero], self.term_names[nonzero])\n",
    "#             }\n",
    "#             policy[self._action_name(a)] = dict(sorted(terms.items(), key=lambda kv: -abs(kv[1])))\n",
    "#         return policy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_2048",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
